\documentclass[12pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{multicol}

\usepackage{enumitem}


\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  urlcolor=cyan,
  linkbordercolor={0 0 1}
}
 
\renewcommand\lstlistingname{Algorithm}
\renewcommand\lstlistlistingname{Algorithms}
\def\lstlistingautorefname{Alg.}

\lstdefinestyle{Python}{
    language        = Python,
    frame           = lines, 
    basicstyle      = \footnotesize,
    keywordstyle    = \color{blue},
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}

% Edit these as appropriate
\newcommand\course{PHYS 243}
\newcommand\hwnumber{5}                  % <-- homework number
\newcommand\MyName{Dominique Acuna}           % <-- My name

\pagestyle{fancyplain}
\headheight 35pt
\lhead{\MyName}

\chead{\textbf{\Large Homework \hwnumber}}
\rhead{\course \\ Deadline:  08/16/2019, 11:59 pm}
\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\begin{document}
\section*{Find the income using Support Vector Machines!}
From the link to \href{https://abtinshahidi.github.io/files/adult.zip}{adult.zip} download the data set. First, take a look at the data. You can see that the data contains categorical data as well. First of run a \textbf{random forests} and \textbf{measure your performance}. 

import numpy as np
import random
import matplotlib.pyplot as plt

# use LaTeX fonts in the plot
plt.rc('text', usetex=True)

# ticks settings
plt.rc('xtick',labelsize=10)
plt.rc('ytick',labelsize=10)

# axes setting
plt.rc('axes', titlesize=22, labelsize=18)     # fontsize of the axes title, labels

# Set the font parameters
plt.rc('font', **font)

# Set the figure parameters
plt.rc("figure", **figure)


adult_data = "H:\Tuition Assistance&Masters\UCR\PHY243\adult.zip\adult.data"

attr_names = ["Age", "WorkClass", "Fnlwgt", "Education", "Education-Num", "Martial-Status", "Occupation", "Relationship", "Race", "Sex", "Capital-Gain", "Capital-Loss", "Hours-per-Week", "Native-Country", "Prediciton"]

AD_dataset = Data_Set(name = "Adult dataset",
                      target_attribute=15,
                      attribute_names=attr_names,
                      file_info=(adult_data, None, ","))

tree_AD = Decision_Tree_Learner(AD_dataset)

tree_AD.display_out()

Test Age
 Age = 0-25 ---> Test WorkClass
      WorkClass = 1-4 ---> Test Fnlwgt
           Fnlwgt = 1.0 ---> Test Education
                Education = recurrence-events ---> Test Education-Num
                     Education-Num = 20-29 ---> RESULT = yes
                     Education-Num = 70-79 ---> RESULT = no
                     Education-Num = 50-59 ---> RESULT = yes
                     Education-Num = 40-49 ---> RESULT = yes
                     Education-Num = 60-69 ---> RESULT = yes
                     Education-Num = 30-39 ---> RESULT = no

                Education = no-recurrence-events ---> RESULT = no

           Fnlwgt = 2.0 ---> Test Fnlwgt
                Education = 1---> RESULT = no
                Education = 2---> RESULT = no
                Education = 3 ---> Test Education-Num
                     Education-Num = 20-29 ---> RESULT = no
                     Education-Num = 70-79 ---> RESULT = no
                     Education-Num = 50-59 ---> RESULT = no
                     Education-Num = 40-49 ---> RESULT = yes
                     Education-Num = 60-69 ---> RESULT = no
                     Education-Num = 30-39 ---> RESULT = no

                Education-Num = ? ---> RESULT = no
                Education-Num = 1 ---> Test Marital-Status
                     Marital-Status = 1 ---> Test Occupation
                          Occupation= 1 ---> RESULT = yes
                          Occupation = 2 ---> Test Relationship
                               Relationship= 1 ---> Test Race
                                    Race = 20-29 ---> RESULT = no
                                    Race = 70-79 ---> RESULT = no
                                    Race = 50-59 ---> RESULT = no
                                    Race = 40-49 ---> RESULT = yes
                                    Race = 60-69 ---> Test Sex
                                         Sex = 1 ---> RESULT = yes
                                         Sex = 2 ---> RESULT = no
                                         Sex = ? ---> RESULT = yes

                                    Race = 30-39 ---> RESULT = yes

                               Relationship = right ---> RESULT = no


                     Marital-Status = 2 ---> RESULT = no
                     Marital-Status = 3 ---> RESULT = yes

                Education-Num = 2 ---> RESULT = yes

           Fnlwgt = 3.0 ---> Test breast-quad
                Education-Num  = 1 ---> RESULT = no
                Education-Num  = 2 ---> RESULT = no
                Education-Num  = 3 ---> Test Class
                     Marital-Status = recurrence-events ---> RESULT = yes
                     Marital-Status = no-recurrence-events ---> RESULT = no

                Education-Num  = ? ---> RESULT = no
                Education-Num  = 4 ---> RESULT = no
                Education-Num  = 5---> RESULT = no


      WorkClass = 5-8 ---> Test Fnlwgt
           Fnlwgt = 1 ---> RESULT = no
           Fnlwgt = 2 ---> RESULT = yes
           Fnlwgt = ? ---> RESULT = no

     
 Age= 30-50 ---> Test WorkClass
      WorkClass = 30-34 ---> Test Age
           Age = 20-29 ---> RESULT = no
           Age = 70-79 ---> RESULT = no
           Age = 50-59 ---> Test Education-Num
                Education-Num = 1 ---> RESULT = no
                Education-Num = 2 ---> RESULT = yes
                Education-Num = 3 ---> RESULT = no

           Age = 40-49 ---> RESULT = no
           Age = 60-69 ---> Test Education-Num
                Education-Num = 1 ---> RESULT = yes
                Education-Num = 2 ---> RESULT = yes
                Education-Num = 3 ---> RESULT = yes
                Education-Num = ? ---> RESULT = yes
                Education-Num = 4 ---> RESULT = no
                Education-Num = 5 ---> RESULT = no

           Age = 30-39 ---> RESULT = yes

 #Need more time for decision tree. Was using breast cancer example but got confused and ran out of time.
AD_Train, AD_test =AD_dataset.train_test_split(test_fraction=0.2, Seed=90)
RF_AD = Random_Forest(AD_Train, n = 100)
def Measure_accuracy(true_values, predictions):
    _sum_ = 0
    for truth, prediction in zip(true_values, predictions):
        if truth==prediction:
            _sum_+=1
    return _sum_/len(predictions)
rf_predictions = [RF_AD(a[:-1]) for a in AD_test.examples]
true_Test_values = [example[AD_test.target_attribute] for example in AD_test.examples]

Measure_accuracy(true_Test_values, rf_predictions)

def plot_the_accuracy(train_set, test_set, list_of_tree_num = np.arange(10, 100, 10)):
    accuracy =[]
    for n in list_of_tree_num:
        RF_AD = Random_Forest(train_set, n=n)
        _predictions = [RF_AD(a[:-1]) for a in test_set.examples]
        _true_values = [example[test_set.target_attribute] for example in test_set.examples]
        accuracy.append(Measure_accuracy(_true_values, _predictions))

    fig = plt.plot(list_of_tree_num, accuracy)
    return fig, accuracy

acc, fig = plot_the_accuracy(AD_Train, AD_test, list(range(5, 60)))

plt.title(r"Accuracy for Random Forests predctions", fontsize=15)
plt.xlabel(r"Number of trees", fontsize=12)
plt.ylabel(r"Accuracy", fontsize=12)
Text(0, 0.5, 'Accuracy')


\subsection*{Transform Categorical data into numbers!}
 Now that we have more complicated algorithm, let's make use out of them. But first you should change your categorical data into real valued number which are needed for the SVM algorithm. Come up with a method that can do this translation. 


cat_df_adults_lc['WorkClass'] = cat_df_adults_lc['WorkClass'].cat.codes
cat_df_adults_lc.head() #alphabetically labeled from 0 to 10


cat_df_adults_lc['Education'] = cat_df_adults_lc['Education'].cat.codes
cat_df_adults_lc.head() #alphabetically labeled from 0 to 10

cat_df_adults_lc['Martial-Status'] = cat_df_adults_lc['Martial-Status'].cat.codes
cat_df_adults_lc.head() #alphabetically labeled from 0 to 10

cat_df_adults_lc['Occupation'] = cat_df_adults_lc['Occupation'].cat.codes
cat_df_adults_lc.head() #alphabetically labeled from 0 to 10

cat_df_adults_lc['Occupation'] = cat_df_adults_lc['Occupation'].cat.codes
cat_df_adults_lc.head() #alphabetically labeled from 0 to 10

cat_df_adults_lc['Protective-syerv'] = cat_df_adults_lc['Protective-syerv'].cat.codes
cat_df_adults_lc.head() #alphabetically labeled from 0 to 10

cat_df_adults_lc['Relationship'] = cat_df_adults_lc['Relationship'].cat.codes
cat_df_adults_lc.head() #alphabetically labeled from 0 to 10

cat_df_adults_lc['Race'] = cat_df_adults_lc['Race'].cat.codes
cat_df_adults_lc.head() #alphabetically labeled from 0 to 10

cat_df_adults_lc['Sex'] = cat_df_adults_lc['Sex'].cat.codes
cat_df_adults_lc.head() #alphabetically labeled from 0 to 10

cat_df_adults_lc['Native-Country'] = cat_df_adults_lc['Native-Country'].cat.codes
cat_df_adults_lc.head() #alphabetically labeled from 0 to 10


\subsection*{Scale your attributes!}
 Now that you have numerical attributes, scale all your features to something reasonable.
 

X = adults.data
y = adults.test
# standardize the data attributes
standardized_X = preprocessing.scale(X)

\subsection*{Run Support Vector Machine!}
Now that you preprocessed your data, you can run the algorithm and measure your performance. Compare your results to the random forest performance on the same task.

from sklearn import svm
def _plot_skitlearn_svm(kernel = "linear", gamma=0.1, degree=2, coef=0):
    svm_classifier = svm.SVC(kernel=kernel, gamma=gamma, degree=degree, coef0 = coef)
    X = adults.data
    y = adults.test
    svm_classifier.fit(X, y)

    # plot the predictions
    x = np.linspace(-1, 2.5, 80)
    y = np.linspace(-1, 2.5, 80)
    x, y = np.meshgrid(x, y)

    x = x.ravel()
    y = y.ravel()

    X_testing = np.c_[x, y]

    predictions = svm_classifier.predict(X_testing)

    colors = []

    for i in predictions:
        if i==1:
            colors.append("r")
        else:
            colors.append("b")

    np.random.seed(20)

    x1 = 1.2 + 0.5 * np.random.randn(20)
    y1 = 1.5 + 0.2 * np.random.randn(20)

    x2 = 0 + 0.3 * np.random.randn(20)
    y2 = 0 + 0.4 * np.random.randn(20)


    fig = plt.scatter(x, y, c=colors, s = 0.5)

    plt.plot(x1, y1, "r.", markersize=10)
    plt.plot(x2, y2, "b.", markersize=10)
    return fig
plt.figure(figsize=(17, 4))


kernels = ["linear", "rbf", "poly", "sigmoid"]

counter = 141

for kernel in kernels:
    plt.subplot(counter); _plot_skitlearn_svm(kernel=kernel); plt.title(r"SVM with {} kernel".format(kernel), fontsize =12)
    counter+=1


\end{document}

